{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93cd44a6",
   "metadata": {},
   "source": [
    "### Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "raw",
   "id": "bab161ec",
   "metadata": {},
   "source": [
    "Ans:Min-max scaling, or normalization, is a data preprocessing technique that rescales numerical features to a fixed range, typically between 0 and 1. It is used to bring all features onto a similar scale, preventing one feature from dominating others due to magnitude differences. The formula for min-max scaling is (X - X_min) / (X_max - X_min).\n",
    "\n",
    "For example, let's consider a dataset of students' exam scores ranging from 60 to 95. Applying min-max scaling, the minimum score (X_min) would be 60 and the maximum score (X_max) would be 95. By applying the formula, each score is scaled to a range of 0 to 1. The purpose is to make the scores comparable and allow for effective analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20106f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-Max scaling: [0.         0.42857143 0.57142857 0.85714286 1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "original_scores = np.array([60, 75, 80, 90, 95])\n",
    "\n",
    "_min = np.min(original_scores)\n",
    "X_max = np.max(original_scores)\n",
    "\n",
    "scaled_scores = (original_scores - X_min) / (X_max - X_min)\n",
    "\n",
    "print(\"Min-Max scaling:\",scaled_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2aab2af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.42857143],\n",
       "       [0.57142857],\n",
       "       [0.85714286],\n",
       "       [1.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "original_scores = np.array([60, 75, 80, 90, 95])\n",
    "\n",
    "reshaped_scores = original_scores.reshape(-1, 1)\n",
    "\n",
    "min_max=MinMaxScaler()\n",
    "\n",
    "min_max.fit(reshaped_scores)\n",
    "\n",
    "min_max.transform(reshaped_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9614322b",
   "metadata": {},
   "source": [
    "### Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a768b130",
   "metadata": {},
   "source": [
    "Ans:The Unit Vector technique, also known as normalization or feature scaling, rescales the values of a feature to have a unit norm or length. It differs from Min-Max scaling in that it focuses on the direction or orientation of the data rather than the range.\n",
    "\n",
    "The formula for unit vector scaling is as follows:\n",
    "\n",
    "X_scaled = X / ||X||\n",
    "\n",
    "where X represents the original feature values, X_scaled represents the scaled feature values, and ||X|| denotes the Euclidean norm or length of the feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b98e479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df=sns.load_dataset('iris')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "001df36e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.803773</td>\n",
       "      <td>0.551609</td>\n",
       "      <td>0.220644</td>\n",
       "      <td>0.031521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.828133</td>\n",
       "      <td>0.507020</td>\n",
       "      <td>0.236609</td>\n",
       "      <td>0.033801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.805333</td>\n",
       "      <td>0.548312</td>\n",
       "      <td>0.222752</td>\n",
       "      <td>0.034269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.800030</td>\n",
       "      <td>0.539151</td>\n",
       "      <td>0.260879</td>\n",
       "      <td>0.034784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.790965</td>\n",
       "      <td>0.569495</td>\n",
       "      <td>0.221470</td>\n",
       "      <td>0.031639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.721557</td>\n",
       "      <td>0.323085</td>\n",
       "      <td>0.560015</td>\n",
       "      <td>0.247699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.729654</td>\n",
       "      <td>0.289545</td>\n",
       "      <td>0.579090</td>\n",
       "      <td>0.220054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.716539</td>\n",
       "      <td>0.330710</td>\n",
       "      <td>0.573231</td>\n",
       "      <td>0.220474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.674671</td>\n",
       "      <td>0.369981</td>\n",
       "      <td>0.587616</td>\n",
       "      <td>0.250281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.690259</td>\n",
       "      <td>0.350979</td>\n",
       "      <td>0.596665</td>\n",
       "      <td>0.210588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width\n",
       "0        0.803773     0.551609      0.220644     0.031521\n",
       "1        0.828133     0.507020      0.236609     0.033801\n",
       "2        0.805333     0.548312      0.222752     0.034269\n",
       "3        0.800030     0.539151      0.260879     0.034784\n",
       "4        0.790965     0.569495      0.221470     0.031639\n",
       "..            ...          ...           ...          ...\n",
       "145      0.721557     0.323085      0.560015     0.247699\n",
       "146      0.729654     0.289545      0.579090     0.220054\n",
       "147      0.716539     0.330710      0.573231     0.220474\n",
       "148      0.674671     0.369981      0.587616     0.250281\n",
       "149      0.690259     0.350979      0.596665     0.210588\n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(normalize(df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]),\n",
    "             columns=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b5acbd",
   "metadata": {},
   "source": [
    "### Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "raw",
   "id": "390910cb",
   "metadata": {},
   "source": [
    "Ans:PCA (Principal Component Analysis) is a statistical technique used for dimensionality reduction. It identifies the most important axes, known as principal components, in a high-dimensional dataset to create a lower-dimensional representation. It achieves this by capturing the maximum variance in the data. PCA is useful for simplifying complex datasets, improving computation efficiency, and aiding visualization while retaining essential patterns or trends in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec708e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.36873621 -0.06977632  0.85630117  0.35484246]\n",
      " [ 0.68187006  0.69682861 -0.1867726  -0.12082672]\n",
      " [-0.53692993  0.63566209  0.05426774  0.55199182]]\n",
      "[0.9201008  0.05494812 0.01978256]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "dataset=datasets.load_iris()\n",
    "\n",
    "X=dataset['data']\n",
    "y=dataset['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "## PCA- Transformation\n",
    "pca=PCA(n_components=3)\n",
    "pca\n",
    "\n",
    "X_train=pca.fit_transform(X_train)\n",
    "X_test=pca.transform(X_test)\n",
    "\n",
    "\n",
    "print(pca.components_)\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f036a9f8",
   "metadata": {},
   "source": [
    "### Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "raw",
   "id": "91546bc8",
   "metadata": {},
   "source": [
    "Ans:PCA is a method for feature extraction that identifies the most important directions of variation in the data, known as principal components. It can be used to reduce the dimensionality of the data while preserving the most relevant information. By projecting the data onto the principal components, a new set of transformed features is obtained for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e9df41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.36873621 -0.06977632  0.85630117  0.35484246]\n",
      " [ 0.68187006  0.69682861 -0.1867726  -0.12082672]]\n",
      "[0.9201008  0.05494812]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "dataset=datasets.load_iris()\n",
    "\n",
    "X=dataset['data']\n",
    "y=dataset['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "## PCA- Transformation\n",
    "pca=PCA(n_components=2)\n",
    "pca\n",
    "\n",
    "X_train=pca.fit_transform(X_train)\n",
    "X_test=pca.transform(X_test)\n",
    "\n",
    "\n",
    "print(pca.components_)\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7796921",
   "metadata": {},
   "source": [
    "### Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "raw",
   "id": "8af71c33",
   "metadata": {},
   "source": [
    "Ans:To preprocess the data for building a recommendation system in the context of a food delivery service, Min-Max scaling can be used. Here's a short explanation of how Min-Max scaling can be applied:\n",
    "\n",
    "Min-Max scaling is a technique used to rescale the features of a dataset to a specific range, typically between 0 and 1. In the case of the food delivery service dataset, we can apply Min-Max scaling to normalize the numerical features such as price, rating, and delivery time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d72bd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   price  rating  delivery_time\n",
      "0   0.00  0.8125           0.25\n",
      "1   0.50  0.0000           1.00\n",
      "2   1.00  1.0000           0.00\n",
      "3   0.25  0.4375           0.75\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = {'price': [10, 20, 30, 15],\n",
    "        'rating': [4.5, 3.2, 4.8, 3.9],\n",
    "        'delivery_time': [30, 45, 25, 40]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "df_scaled = pd.DataFrame(scaled_data, columns=df.columns)\n",
    "\n",
    "print(df_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fa93a2",
   "metadata": {},
   "source": [
    "### Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "raw",
   "id": "cf98fcbf",
   "metadata": {},
   "source": [
    "Ans:When working on a project to predict stock prices with a dataset that contains numerous features, PCA (Principal Component Analysis) can be used to reduce the dimensionality of the dataset. Here's an explanation of how PCA can be applied:\n",
    "\n",
    "1.Preprocess the dataset by handling missing values, normalizing or standardizing features, and ensuring suitable data format.\n",
    "\n",
    "2.Select relevant features for stock price prediction using techniques like correlation analysis or domain knowledge.\n",
    "\n",
    "3.Apply PCA to further reduce dimensionality by finding directions of maximum variance (principal components) and projecting the data onto them.\n",
    "\n",
    "4.Determine the number of principal components to retain based on the explained variance, typically aiming for a significant portion (e.g., 90% or 95%).\n",
    "\n",
    "5.Transform the dataset by projecting onto the selected principal components, effectively reducing dimensionality.\n",
    "\n",
    "6.Use the reduced-dimensional dataset as input for training a stock price prediction model using machine learning algorithms.\n",
    "\n",
    "7.PCA helps eliminate redundant or less informative features, simplifies the dataset, and improves computational efficiency for building more accurate prediction models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "08ed6bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.13619740e+00  5.00117142e-02]\n",
      " [ 1.62864670e+00 -9.63048695e-02]\n",
      " [-3.51654987e-20 -4.96506706e-17]\n",
      " [-1.62864670e+00  9.63048695e-02]\n",
      " [-3.13619740e+00 -5.00117142e-02]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = {\n",
    "    'stock_price': [100, 110, 120, 130, 140],\n",
    "    'revenue': [100000, 120000, 150000, 180000, 200000],\n",
    "    'profit': [20000, 25000, 30000, 35000, 40000],\n",
    "    'debt': [50000, 40000, 30000, 20000, 10000],\n",
    "    'market_trend_1': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    'market_trend_2': [0.5, 0.4, 0.3, 0.2, 0.1]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df.drop('stock_price', axis=1)\n",
    "y = df['stock_price']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=2)  \n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(X_pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db02bddd",
   "metadata": {},
   "source": [
    "### Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00bd30a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "original_scores = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "reshaped_scores = original_scores.reshape(-1, 1)\n",
    "\n",
    "min_max = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "scaled_scores = min_max.fit_transform(reshaped_scores)\n",
    "\n",
    "print(scaled_scores.flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7335cf3",
   "metadata": {},
   "source": [
    "### Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "76ee90e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio: [7.86997282e-01 1.56971400e-01 4.76276032e-02 8.40371432e-03\n",
      " 1.81905527e-33]\n",
      "Cumulative Variance Ratio: [0.78699728 0.94396868 0.99159629 1.         1.        ]\n",
      "Number of Principal Components to Retain: 3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare the dataset\n",
    "data = {\n",
    "    'height': [170, 165, 180, 155, 175],\n",
    "    'weight': [65, 55, 70, 45, 68],\n",
    "    'age': [30, 25, 35, 28, 32],\n",
    "    'gender': [0, 1, 1, 0, 1],\n",
    "    'blood_pressure': [120, 118, 122, 115, 123]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "features = df[['height', 'weight', 'age', 'gender', 'blood_pressure']]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(features)\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(scaled_data)\n",
    "\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Determine the number of principal components to retain\n",
    "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "num_components = np.argmax(cumulative_variance_ratio >= 0.95) + 1\n",
    "\n",
    "print(\"Explained Variance Ratio:\", explained_variance_ratio)\n",
    "print(\"Cumulative Variance Ratio:\", cumulative_variance_ratio)\n",
    "print(\"Number of Principal Components to Retain:\", num_components)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b3cf0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
