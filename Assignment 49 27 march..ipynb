{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e3bf56a",
   "metadata": {},
   "source": [
    "### Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d9f361eb",
   "metadata": {},
   "source": [
    "Ans:R-squared, also known as the coefficient of determination, is a statistical measure used to assess the goodness of fit of a linear regression model. It indicates the proportion of the variance in the dependent variable that can be explained by the independent variables in the model. In simpler terms, it tells us how well the regression line fits the actual data points.\n",
    "\n",
    "R-squared is calculated by comparing the total sum of squares (TSS) and the residual sum of squares (RSS) of the regression model. The TSS represents the total variability of the dependent variable, while the RSS measures the unexplained variability or the sum of squared residuals (the differences between the observed values and the predicted values).\n",
    "\n",
    "The formula for calculating R-squared is:\n",
    "\n",
    "R-squared = 1 - (RSS / TSS)\n",
    "\n",
    "R-squared ranges between 0 and 1. A value of 0 indicates that the model explains none of the variability in the dependent variable, while a value of 1 suggests that the model explains all the variability. In practical terms, higher R-squared values indicate a better fit of the regression line to the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "660a4e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope (Coefficient): 4999.999999999999\n",
      "Intercept: 25000.000000000004\n",
      "R-squared: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "years_experience = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1)  # Independent variable\n",
    "salary = np.array([30000, 35000, 40000, 45000, 50000, 55000, 60000, 65000, 70000, 75000])  # Dependent variable\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(years_experience, salary)\n",
    "\n",
    "slope = model.coef_[0]\n",
    "intercept = model.intercept_\n",
    "\n",
    "print(\"Slope (Coefficient):\", slope)\n",
    "print(\"Intercept:\", intercept)\n",
    "\n",
    "y_pred = model.predict(years_experience)\n",
    "\n",
    "r_squared = r2_score(salary, y_pred)\n",
    "print(\"R-squared:\", r_squared)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00342ad",
   "metadata": {},
   "source": [
    "### Q2. Define adjusted R-squared and explain how it differs from the regular R-squared. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "c13389dc",
   "metadata": {},
   "source": [
    "Ans:Adjusted R-squared is a modified version of the regular R-squared that takes into account the number of independent variables in a linear regression model. It addresses the issue of overfitting by penalizing the inclusion of irrelevant or redundant variables in the model.\n",
    "\n",
    "Regular R-squared tends to increase as more independent variables are added to the model, even if those variables do not have a significant impact on the dependent variable. This can give a false sense of model improvement. Adjusted R-squared, on the other hand, adjusts the R-squared value by considering the number of predictors and the sample size.\n",
    "\n",
    "The formula for calculating adjusted R-squared is:\n",
    "\n",
    "Adjusted R2 = 1 – [(1-R2)*(n-1)/(n-k-1)]\n",
    "\n",
    "where:\n",
    "\n",
    "R2: The R2 of the model\n",
    "n: The number of observations\n",
    "k: The number of predictor variables\n",
    "\n",
    "The adjusted R-squared value can range from negative infinity to 1. Like the regular R-squared, a higher adjusted R-squared value indicates a better fit of the model. However, adjusted R-squared penalizes the inclusion of unnecessary variables. It increases only if the addition of a new variable improves the model fit more than would be expected by chance. It decreases if the added variables do not contribute meaningfully to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20aeb811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope (Coefficient): 4999.999999999999\n",
      "Intercept: 25000.000000000004\n",
      "R-squared: 1.0\n",
      "Adjusted R-squared: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "years_experience = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1)  # Independent variable\n",
    "salary = np.array([30000, 35000, 40000, 45000, 50000, 55000, 60000, 65000, 70000, 75000])  # Dependent variable\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(years_experience, salary)\n",
    "\n",
    "slope = model.coef_[0]\n",
    "intercept = model.intercept_\n",
    "\n",
    "print(\"Slope (Coefficient):\", slope)\n",
    "print(\"Intercept:\", intercept)\n",
    "\n",
    "y_pred = model.predict(years_experience)\n",
    "\n",
    "r_squared = r2_score(salary, y_pred)\n",
    "print(\"R-squared:\", r_squared)\n",
    "\n",
    "n = len(years_experience) \n",
    "k = 1\n",
    "\n",
    "adjusted_r_squared = 1 - (1 - r_squared) * (n - 1) / (n - k - 1)\n",
    "print(\"Adjusted R-squared:\", adjusted_r_squared)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2db7e0",
   "metadata": {},
   "source": [
    "### Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "842c5d0e",
   "metadata": {},
   "source": [
    "Ans:The key points regarding the appropriate use of adjusted R-squared:\n",
    "\n",
    "1.Model comparison: Adjusted R-squared is useful for comparing regression models with different numbers of predictors, providing a fairer comparison than regular R-squared.\n",
    "\n",
    "2.Variable selection: Adjusted R-squared helps in selecting relevant variables for the regression model by indicating their contribution beyond chance.\n",
    "\n",
    "3.Model complexity: Adjusted R-squared considers the complexity of the model by penalizing the inclusion of unnecessary variables.\n",
    "\n",
    "4.Sample size considerations: Adjusted R-squared accounts for sample size and avoids inflated values that regular R-squared may give for smaller datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6990211c",
   "metadata": {},
   "source": [
    "### Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c224eb13",
   "metadata": {},
   "source": [
    "Ans:RMSE (Root Mean Squared Error):RMSE is the square root of the average of the squared differences between the predicted and actual values. It measures the average magnitude of the residuals or prediction errors. RMSE is calculated using the following\n",
    "\n",
    "RMSE = √(1/n) * Σ(y_actual - y_predicted)^2\n",
    "\n",
    "Where:\n",
    "\n",
    " .n is the number of data points\n",
    " .y_actual represents the actual values of the dependent variable\n",
    " .y_predicted represents the predicted values of the dependent variable\n",
    "\n",
    "RMSE provides a measure of the standard deviation of the residuals, and it is in the same unit as the dependent variable. Lower values of RMSE indicate better model performance, with 0 being a perfect fit.\n",
    "\n",
    "2.MSE (Mean Squared Error):MSE is the average of the squared differences between the predicted and actual values. It measures the average squared residual. MSE is calculated as follows\n",
    "\n",
    "MSE = (1/n) * Σ(y_actual - y_predicted)^2\n",
    "\n",
    "Where:\n",
    "\n",
    " .n is the number of data points\n",
    " .y_actual represents the actual values of the dependent variable\n",
    " .y_predicted represents the predicted values of the dependent variable\n",
    "\n",
    "MSE provides an average measure of the squared residuals, and it is useful for comparing different models. However, since it is in squared units, it may not be directly interpretable in the original scale.\n",
    "\n",
    "3.MAE (Mean Absolute Error):MAE is the average of the absolute differences between the predicted and actual values. It measures the average absolute residual. MAE is calculated as follows\n",
    "\n",
    "MAE = (1/n) * Σ|y_actual - y_predicted|\n",
    "\n",
    "Where:\n",
    "\n",
    " .n is the number of data points\n",
    " .y_actual represents the actual values of the dependent variable\n",
    " .y_predicted represents the predicted values of the dependent variable\n",
    "\n",
    "MAE provides an average measure of the absolute residuals and is in the same unit as the dependent variable. It is less sensitive to outliers compared to RMSE.    \n",
    "\n",
    "These metrics are used to assess the accuracy and performance of regression models. Lower values of RMSE, MSE, and MAE indicate better model fit and predictive accuracy. The choice of which metric to use depends on the specific context and requirements of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad614a31",
   "metadata": {},
   "source": [
    "### Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis."
   ]
  },
  {
   "cell_type": "raw",
   "id": "909542d3",
   "metadata": {},
   "source": [
    "Ans:Advantages:\n",
    "\n",
    "RMSE:\n",
    ".Provides a measure of the standard deviation of the residuals.\n",
    ".Sensitivity to outliers.\n",
    ".Suitable for optimization algorithms.\n",
    ".It will be in the smae unit.\n",
    ".Equation is differrntiable\n",
    "\n",
    "MSE:\n",
    "\n",
    ".Provides an overall measure of performance.\n",
    ".Suitable for mathematical calculations and optimization.\n",
    ".Equation is differrntiable\n",
    ".It has only one local or global minima\n",
    "\n",
    "MAE:\n",
    ".Robust to outliers.\n",
    ".Provides an intuitive understanding of average absolute deviation.\n",
    ".It will be in the smae unit.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "RMSE:\n",
    ".Influence of outliers.\n",
    ".Not directly interpretable in original units.\n",
    "\n",
    "MSE:\n",
    ".Sensitivity to outliers.\n",
    ".Not in original units.\n",
    "\n",
    "MAE:\n",
    ".Treats all errors equally.\n",
    ".Not differentiable at zero.\n",
    ".Convergers usually takes more time\n",
    "\n",
    "RMSE, MSE, and MAE have their own advantages and disadvantages. The choice of metric depends on the specific requirements of the regression analysis, such as the presence of outliers, the need for interpretability, and the optimization algorithms being used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914f473c",
   "metadata": {},
   "source": [
    "### Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0213aab1",
   "metadata": {},
   "source": [
    "Ans:Lasso regularization, also known as L1 regularization, is a technique used in linear regression and other models to prevent overfitting and improve generalization by adding a penalty term to the loss function. It gets its name from \"L1 norm,\" which is the sum of the absolute values of the model's coefficients.\n",
    "\n",
    "In Lasso regularization, the penalty term is proportional to the absolute value of the coefficients, multiplied by a regularization parameter (lambda or alpha). The objective is to minimize the sum of the squared errors between the predicted and actual values, along with the penalty term. The penalty encourages the model to shrink the coefficients of less important features to zero, effectively performing feature selection by eliminating irrelevant features.\n",
    "\n",
    "The key difference between Lasso and Ridge regularization (L2 regularization) lies in the penalty term. While Lasso uses the absolute values of the coefficients (L1 norm) as the penalty term, Ridge uses the squared values (L2 norm). As a result, Lasso has the ability to perform feature selection by driving some coefficients to exactly zero, effectively excluding those features from the model. Ridge, on the other hand, only shrinks the coefficients towards zero but does not eliminate them entirely.\n",
    "\n",
    "Lasso regularization is more appropriate when there is a belief that only a small subset of features is truly relevant to the target variable. By zeroing out irrelevant features, Lasso can simplify the model and enhance interpretability. It is particularly useful when dealing with high-dimensional datasets or when there is multicollinearity among the features. However, if all features are considered potentially important, or when the goal is not feature selection but rather reducing the impact of large coefficient values, Ridge regularization may be more suitable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b0971e",
   "metadata": {},
   "source": [
    "### Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate."
   ]
  },
  {
   "cell_type": "raw",
   "id": "28e0a8a0",
   "metadata": {},
   "source": [
    "Ans:\n",
    "Regularized linear models help prevent overfitting in machine learning by introducing a penalty term to the loss function that discourages complex or large parameter values. This penalty encourages the model to find a balance between fitting the training data well and keeping the model's parameters small, reducing the risk of overfitting.\n",
    "\n",
    "One common type of regularized linear model is ridge regression, also known as Tikhonov regularization. In ridge regression, a regularization term is added to the ordinary least squares (OLS) loss function. The regularization term is proportional to the sum of squared values of the model's coefficients multiplied by a hyperparameter called the regularization parameter, typically denoted as λ.\n",
    "\n",
    "The modified loss function for ridge regression can be written as:\n",
    "\n",
    "Loss = OLS Loss + λ * (sum of squared coefficients)\n",
    "\n",
    "The effect of the regularization term is that it penalizes large coefficient values. When λ is large, the model is strongly regularized, and the coefficients are pushed towards zero. When λ is small or zero, the model behaves more like traditional linear regression.\n",
    "\n",
    "Here's an example to illustrate how ridge regression helps prevent overfitting:\n",
    "\n",
    "Suppose you have a dataset with 100 observations and 10 features. You fit a linear regression model to the data, and it achieves perfect accuracy on the training set (all observations are correctly predicted). However, when you evaluate the model on a test set, it performs poorly, suggesting overfitting. write the python codee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "427707db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.6\n",
      "Test Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(100, 10)  # Features\n",
    "y = np.random.randint(0, 2, 100)  # Target variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "ridge = Ridge(alpha=1.0)  # alpha is the regularization parameter\n",
    "\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_train_pred = ridge.predict(X_train_scaled)\n",
    "y_train_pred_rounded = np.round(y_train_pred)\n",
    "\n",
    "training_accuracy = accuracy_score(y_train, y_train_pred_rounded)\n",
    "print(\"Training Accuracy:\", training_accuracy)\n",
    "\n",
    "y_test_pred = ridge.predict(X_test_scaled)\n",
    "y_test_pred_rounded = np.round(y_test_pred)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred_rounded)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd0eda8",
   "metadata": {},
   "source": [
    "### Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis."
   ]
  },
  {
   "cell_type": "raw",
   "id": "9e08315a",
   "metadata": {},
   "source": [
    "Ans:Here are the four main points summarizing the limitations of regularized linear models:\n",
    "\n",
    "1.Limited applicability to nonlinear relationships: Regularized linear models assume a linear relationship between the features and the target variable, which can limit their ability to capture complex nonlinear patterns in the data.\n",
    "\n",
    "2.Sensitivity to feature scaling: Regularized linear models are sensitive to the scale of the features, requiring them to be standardized or normalized before applying regularization. This extra preprocessing step adds complexity to the analysis.\n",
    "\n",
    "3.Challenges in feature interpretability: Regularization techniques like ridge regression can shrink coefficients towards zero, making it more challenging to interpret the individual effects of features on the target variable.\n",
    "\n",
    "4.Inability to perform variable selection: Regularization does not automatically select relevant features; it only shrinks coefficients towards zero. This means that even with strong regularization, all features may still be retained to some extent, potentially including irrelevant or redundant features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100a9f8b",
   "metadata": {},
   "source": [
    "### Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "56f42454",
   "metadata": {},
   "source": [
    "Ans:Based on the given information, Model B with an MAE (Mean Absolute Error) of 8 would be chosen as the better performer. The lower MAE indicates that, on average, the absolute difference between the predicted values and the actual values is smaller in Model B compared to Model A.\n",
    "\n",
    "While the choice of metric depends on the specific context and goals of the analysis, there are a few limitations to consider when relying solely on these metrics:\n",
    "\n",
    "1.Sensitivity to outliers: Both RMSE (Root Mean Square Error) and MAE are sensitive to outliers. If the dataset contains extreme outliers, these metrics may not accurately represent the overall performance of the model.\n",
    "\n",
    "2.Scale dependence: RMSE is influenced by the scale of the target variable since it squares the errors, making it more sensitive to large errors. MAE, on the other hand, is not affected by the scale. Therefore, the choice of metric may depend on the scale and distribution of the target variable.\n",
    "\n",
    "3.Interpretability: While both metrics provide a measure of prediction accuracy, they do not provide direct insights into the direction and magnitude of the errors. Other evaluation techniques, such as residual analysis or visualizations, can help gain a deeper understanding of the model's performance.\n",
    "\n",
    "4.Importance of individual predictions: If the importance of individual predictions varies, such as in cases where larger errors have higher consequences, choosing the better model based solely on these metrics may not fully capture the desired performance criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07da3430",
   "metadata": {},
   "source": [
    "### Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "af56384c",
   "metadata": {},
   "source": [
    "Ans:Choosing the better performer between Model A (Ridge regularization with a regularization parameter of 0.1) and Model B (Lasso regularization with a regularization parameter of 0.5) depends on the specific goals and requirements of the analysis. Here are some considerations to help with the decision:\n",
    "\n",
    "1.Ridge regularization (L2 regularization) encourages small but non-zero coefficients. It is effective in handling multicollinearity and can be useful when all features are potentially relevant. Ridge regularization tends to result in more stable and robust models.\n",
    "\n",
    "2.Lasso regularization (L1 regularization) encourages sparse solutions by pushing some coefficients to exactly zero. This makes Lasso regularization suitable for feature selection, as it can effectively eliminate irrelevant features from the model. Lasso can be particularly useful when dealing with high-dimensional datasets.\n",
    "\n",
    "To determine the better performer, you would need to evaluate the models based on specific criteria such as predictive accuracy, feature importance, or interpretability. Additionally, it's important to consider any trade-offs or limitations associated with each regularization method:\n",
    "\n",
    "1.Ridge regularization does not perform variable selection, meaning all features will have non-zero coefficients to some extent. If feature interpretability is important, Ridge regularization may be preferred as it retains all features and their associated coefficients.\n",
    "\n",
    "2.Lasso regularization performs automatic feature selection by driving some coefficients to zero. This can lead to a more interpretable model with a smaller subset of relevant features. However, Lasso may struggle when dealing with highly correlated features, as it tends to select only one of them while discarding the others.\n",
    "\n",
    "3.The choice of the regularization parameter (lambda) is crucial. A larger value of lambda increases the strength of regularization and can shrink coefficients more towards zero, while a smaller value reduces the impact of regularization. It's important to tune the regularization parameter using cross-validation or other techniques to find the optimal balance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
